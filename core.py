import functools
import gc
import os
from typing import List, Optional

import einops
import gradio as gr
import torch
from transformer_lens import HookedTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM


def clear_gpu_cache():
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


def get_instructions(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        instructions = file.readlines()

    instructions = [line.strip() for line in instructions]
    return instructions


def tokenize_instructions(tokenizer, instructions):
    prompts = [tokenizer.apply_chat_template(
        [{"role": "user", "content": instruction}],
        tokenize=False,
        add_generation_prompt=True
    ) for instruction in instructions]
    return tokenizer(prompts, padding=True, truncation=False, return_tensors="pt").input_ids


def get_orthogonalized_matrix(matrix, vec):
    proj = einops.einsum(matrix, vec.view(-1, 1), '... d_model, d_model single -> ... single') * vec
    return matrix - proj


def get_diverted_matrix(matrix, vec):
    # è®¡ç®—æ¯ä¸€è¡Œä¸å‘é‡ v çš„ç‚¹ç§¯
    dot_products = torch.matmul(matrix, vec)

    # å°†ç‚¹ç§¯ç»“æœæ‰©å±•ä¸ºä¸ A ç›¸åŒçš„å½¢çŠ¶
    dot_products_expanded = dot_products.unsqueeze(1).expand_as(matrix)

    # å°†æ‰©å±•åçš„ç‚¹ç§¯ç»“æœä¸åŸçŸ©é˜µ A ç›¸åŠ 
    A_adjusted = matrix + dot_products_expanded

    return A_adjusted


def remove_mode(model_name, n_inst_train, refusal_dir_coefficient, layer, device, harmful_behaviors_file_path,
                harmless_behaviors_file_path, progress=gr.Progress()):
    def p(percentage, message):
        print(f"{percentage * 100}%: {message}")
        progress(percentage, message)

    try:
        p(0, "æ­£åœ¨åŠ è½½æ¨¡å‹...")

        model = HookedTransformer.from_pretrained(
            model_name,
            device=device,
            n_devices=1,
            dtype=torch.bfloat16,
            default_padding_side='left'
        )
        model.tokenizer.padding_side = 'left'
        model.tokenizer.pad_token = '<|extra_0|>'

        tokenize_instructions_fn = functools.partial(tokenize_instructions, tokenizer=model.tokenizer)
        model.tokenizer.add_special_tokens({'pad_token': '[PAD]'})

        p(0.1, "æ­£åœ¨åŠ è½½æœ‰å®³æ•°æ®åº“...")
        harmful_inst_train = get_instructions(harmful_behaviors_file_path)

        p(0.15, "æ­£åœ¨åŠ è½½æ— å®³æ•°æ®åº“...")
        harmless_inst_train = get_instructions(harmless_behaviors_file_path)

        p(0.2, "æ­£åœ¨åˆ†è¯æœ‰å®³æ•°æ®åº“...")
        harmful_tokens = tokenize_instructions_fn(instructions=harmful_inst_train[:n_inst_train])

        p(0.25, "æ­£åœ¨åˆ†è¯æ— å®³æ•°æ®åº“...")
        harmless_tokens = tokenize_instructions_fn(instructions=harmless_inst_train[:n_inst_train])

        p(0.3, "æ­£åœ¨è·å–æœ‰å®³æ•°æ®åº“ä¸­é—´è¿‡ç¨‹å‚æ•°...")
        harmful_logits, harmful_cache = model.run_with_cache(harmful_tokens,
                                                             names_filter=lambda hook_name: 'resid' in hook_name)
        harmful_logits = harmful_logits.cpu()
        del harmful_logits
        clear_gpu_cache()

        p(0.5, "æ­£åœ¨è·å–æ— å®³æ•°æ®åº“ä¸­é—´è¿‡ç¨‹å‚æ•°...")
        harmless_logits, harmless_cache = model.run_with_cache(harmless_tokens,
                                                               names_filter=lambda hook_name: 'resid' in hook_name)
        harmless_logits = harmless_logits.cpu()
        del harmless_logits
        clear_gpu_cache()

        p(0.7, "æ­£åœ¨è®¡ç®—æœ‰å®³ä¸æ— å®³æ¿€æ´»å¹³å‡å·®å¼‚...")
        pos = -1

        refusal_dir: List[Optional[torch.Tensor]] = [None for _ in range(len(model.blocks))]

        if layer == "-2":
            layer = [i for i in range(len(model.blocks))]
        elif layer == "-1":
            layer = [int(len(model.blocks) / 2)]
        elif len(layer.split(",")) != 1:
            layer = [int(i) for i in layer.split(",")]
        else:
            layer = [int(layer)]

        if int(len(model.blocks) / 2) not in layer:
            layer.append(int(len(model.blocks) / 2))

        for i in layer:
            harmful_mean_act = harmful_cache['resid_pre', i][:, pos, :].mean(dim=0)
            harmless_mean_act = harmless_cache['resid_pre', i][:, pos, :].mean(dim=0)
            refusal_dir[i] = harmful_mean_act - harmless_mean_act
            refusal_dir[i] = refusal_dir[i] / refusal_dir[i].norm()
            refusal_dir[i] = refusal_dir[i] * refusal_dir_coefficient

        del harmful_cache, harmless_cache
        torch.cuda.empty_cache()

        filtered_tensors = [t for t in refusal_dir if t is not None]
        weights = torch.randn(len(filtered_tensors))
        weights = torch.softmax(weights, dim=0)
        combined_tensor = torch.sum(torch.stack([w * t for w, t in zip(weights, filtered_tensors)]), dim=0)

        p(0.72, "æ­£åœ¨ç§»é™¤åŸæ¨¡å‹å®‰å…¨æ–¹å‘å‘é‡...")

        print(f"æ¨¡å‹æ€»å±‚æ•°ï¼š{len(model.blocks)} å½“å‰æå–ç”¨å±‚æ•°ï¼š{layer}")

        blocks: List[List[Optional[torch.Tensor]]] = [[None, None] for _ in range(len(model.blocks))]

        for i in range(len(model.blocks)):
            blocks[i][0] = get_orthogonalized_matrix(model.blocks[i].attn.W_O, combined_tensor)
            blocks[i][1] = get_orthogonalized_matrix(model.blocks[i].mlp.W_out, combined_tensor)

        model_W_E_data = get_orthogonalized_matrix(model.W_E, combined_tensor)

        p(0.73, "æ­£åœ¨æ¸…ç†å†…å­˜ä¸æ˜¾å­˜...")
        model = model.cpu()
        del model
        clear_gpu_cache()

        p(0.75, "æ­£åœ¨åˆ›å»ºåŸæ¨¡å‹å‰¯æœ¬...")
        save_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=device)

        p(0.95, "æ­£åœ¨å†™å…¥æ–°å‘é‡åˆ°æ–°æ¨¡å‹...")
        with torch.no_grad():
            save_model.model.embed_tokens.weight.data = model_W_E_data.contiguous()
            size = blocks[layer[0]][0].shape[-1]
            for i in range(len(blocks)):
                save_model.model.layers[i].self_attn.o_proj.weight.data = blocks[i][0].view(-1, size).T.contiguous()
                save_model.model.layers[i].mlp.down_proj.weight.data = blocks[i][1].T.contiguous()
        save_model.save_pretrained(f"{model_name}-Without-Refusal")
        AutoTokenizer.from_pretrained(model_name).save_pretrained(f"{model_name}-Without-Refusal")

        p(1, "å®Œæˆ")
        save_model = save_model.cpu()
        gc.collect()
        clear_gpu_cache()

        if 'save_model' in locals():
            del save_model

        print("ğŸ¤— åˆ›å»ºæˆåŠŸ\n\nå·²ä¿å­˜æ¨¡å‹åˆ°" + os.getcwd() + f"/{model_name}-Without-Refusal")
        return "ğŸ¤— åˆ›å»ºæˆåŠŸ\n\nå·²ä¿å­˜æ¨¡å‹åˆ°" + os.getcwd() + f"/{model_name}-Without-Refusal"
    except Exception as e:
        print(e)

        # æ£€æŸ¥å¹¶åˆ é™¤å˜é‡
        for var_name in ['model', 'save_model']:
            if var_name in locals():
                del locals()[var_name]

        # æ¸…ç†æ˜¾å­˜
        clear_gpu_cache()

        return ["ğŸ¤¯ å‘ç”Ÿé”™è¯¯\n\n" + str(e), "å·²æ¸…ç†æ˜¾å¡æ˜¾å­˜å ç”¨"]


def divert_mode(model_name, n_inst_train, target_dir_coefficient, layer, device, target_prompt,
                basic_behaviors_file_path, progress=gr.Progress()):
    def p(percentage, message):
        print(f"{percentage * 100}%: {message}")
        progress(percentage, message)

    try:
        p(0, "æ­£åœ¨åŠ è½½æ¨¡å‹...")

        model = HookedTransformer.from_pretrained(
            model_name,
            device=device,
            n_devices=1,
            dtype=torch.bfloat16,
            default_padding_side='left'
        )
        model.tokenizer.padding_side = 'left'
        model.tokenizer.pad_token = '<|extra_0|>'

        tokenize_instructions_fn = functools.partial(tokenize_instructions, tokenizer=model.tokenizer)
        model.tokenizer.add_special_tokens({'pad_token': '[PAD]'})

        p(0.1, "æ­£åœ¨åŠ è½½æ™®é€šPromptæ•°æ®åº“...")
        from_inst_train = get_instructions(basic_behaviors_file_path)

        to_inst_train = [line + target_prompt for line
                         in
                         from_inst_train]

        p(0.2, "æ­£åœ¨åˆ†è¯æ™®é€šæ•°æ®åº“...")
        from_tokens = tokenize_instructions_fn(instructions=from_inst_train[:n_inst_train])

        p(0.25, "æ­£åœ¨åˆ†è¯ç›®æ ‡æ•°æ®åº“...")
        to_tokens = tokenize_instructions_fn(instructions=to_inst_train[:n_inst_train])

        p(0.3, "æ­£åœ¨è·å–æ™®é€šæ•°æ®åº“ä¸­é—´è¿‡ç¨‹å‚æ•°...")
        from_logits, from_cache = model.run_with_cache(from_tokens,
                                                       names_filter=lambda hook_name: 'resid' in hook_name)
        from_logits = from_logits.cpu()
        del from_logits
        clear_gpu_cache()

        p(0.5, "æ­£åœ¨è·å–ç›®æ ‡æ•°æ®åº“ä¸­é—´è¿‡ç¨‹å‚æ•°...")
        to_logits, to_cache = model.run_with_cache(to_tokens,
                                                   names_filter=lambda hook_name: 'resid' in hook_name)
        to_logits = to_logits.cpu()
        del to_logits
        clear_gpu_cache()

        p(0.7, "æ­£åœ¨è®¡ç®—æ™®é€šä¸ç›®æ ‡æ¿€æ´»å¹³å‡å·®å¼‚...")
        pos = -1

        target_dir: List[Optional[torch.Tensor]] = [None for _ in range(len(model.blocks))]

        if layer == "-2":
            layer = [i for i in range(len(model.blocks))]
        elif layer == "-1":
            layer = [int(len(model.blocks) / 2)]
        elif len(layer.split(",")) != 1:
            layer = [int(i) for i in layer.split(",")]
        else:
            layer = [int(layer)]

        if int(len(model.blocks) / 2) not in layer:
            layer.append(int(len(model.blocks) / 2))

        for i in layer:
            from_mean_act = from_cache['resid_pre', i][:, pos, :].mean(dim=0)
            to_mean_act = to_cache['resid_pre', i][:, pos, :].mean(dim=0)
            target_dir[i] = to_mean_act - from_mean_act
            target_dir[i] = target_dir[i] / target_dir[i].norm()
            target_dir[i] = target_dir[i] * target_dir_coefficient

        del from_mean_act, to_mean_act
        torch.cuda.empty_cache()

        filtered_tensors = [t for t in target_dir if t is not None]
        weights = torch.randn(len(filtered_tensors))
        weights = torch.softmax(weights, dim=0)
        combined_tensor = torch.sum(torch.stack([w * t for w, t in zip(weights, filtered_tensors)]), dim=0)

        p(0.72, "æ­£åœ¨è½¬å‘å‘é‡...")

        print(f"æ¨¡å‹æ€»å±‚æ•°ï¼š{len(model.blocks)} å½“å‰æå–ç”¨å±‚æ•°ï¼š{layer}")

        blocks: List[List[Optional[torch.Tensor]]] = [[None, None] for _ in range(len(model.blocks))]

        for i in range(len(model.blocks)):
            blocks[i][0] = get_diverted_matrix(model.blocks[i].attn.W_O, combined_tensor)
            blocks[i][1] = get_diverted_matrix(model.blocks[i].mlp.W_out, combined_tensor)

        model_W_E_data = get_diverted_matrix(model.W_E, combined_tensor)

        p(0.73, "æ­£åœ¨æ¸…ç†å†…å­˜ä¸æ˜¾å­˜...")
        model = model.cpu()
        del model
        clear_gpu_cache()

        p(0.75, "æ­£åœ¨åˆ›å»ºåŸæ¨¡å‹å‰¯æœ¬...")
        save_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=device)

        p(0.95, "æ­£åœ¨å†™å…¥æ–°å‘é‡åˆ°æ–°æ¨¡å‹...")
        with torch.no_grad():
            save_model.model.embed_tokens.weight.data = model_W_E_data.contiguous()
            size = blocks[layer[0]][0].shape[-1]
            for i in range(len(blocks)):
                save_model.model.layers[i].self_attn.o_proj.weight.data = blocks[i][0].view(-1, size).T.contiguous()
                save_model.model.layers[i].mlp.down_proj.weight.data = blocks[i][1].T.contiguous()
        save_model.save_pretrained(f"{model_name}-Without-Refusal")
        AutoTokenizer.from_pretrained(model_name).save_pretrained(f"{model_name}-Without-Refusal")

        p(1, "å®Œæˆ")
        save_model = save_model.cpu()
        gc.collect()
        clear_gpu_cache()

        if 'save_model' in locals():
            del save_model

        print("ğŸ¤— åˆ›å»ºæˆåŠŸ\n\nå·²ä¿å­˜æ¨¡å‹åˆ°" + os.getcwd() + f"/{model_name}-Diverted")
        return "ğŸ¤— åˆ›å»ºæˆåŠŸ\n\nå·²ä¿å­˜æ¨¡å‹åˆ°" + os.getcwd() + f"/{model_name}-Diverted"
    except Exception as e:
        print(e)

        # æ£€æŸ¥å¹¶åˆ é™¤å˜é‡
        for var_name in ['model', 'save_model']:
            if var_name in locals():
                del locals()[var_name]

        # æ¸…ç†æ˜¾å­˜
        clear_gpu_cache()

        return ["ğŸ¤¯ å‘ç”Ÿé”™è¯¯\n\n" + str(e), "å·²æ¸…ç†æ˜¾å¡æ˜¾å­˜å ç”¨"]
