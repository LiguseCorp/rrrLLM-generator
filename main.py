import functools
import gc
import os
from typing import List, Optional

import einops
import gradio as gr
import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from transformer_lens import HookedTransformer
from transformer_lens.loading_from_pretrained import OFFICIAL_MODEL_NAMES
from transformers import AutoTokenizer, AutoModelForCausalLM
import argparse

parser = argparse.ArgumentParser(description="rrrLLM")
parser.add_argument('--ip', type=str, default="0.0.0.0", help='IP the web interface running on')
parser.add_argument('--port', type=int, default=8080, help='Port the web interface running on')
parser.add_argument('--cli', action="store_true", default=8080, help='Use CLI instead of web interface')
parser.add_argument('--model-name', type=str, default="Qwen/Qwen2-1.5B-Instruct", help='The model to be processed. Default is Qwen/Qwen2-1.5B-Instruct')
parser.add_argument('--n-inst-train', type=int, default=32, help='The number of training instances for refusal direction. Default is 32')
parser.add_argument('--refusal-dir-coefficient', type=float, default=1, help='The coefficient for refusal direction. Default is 1')
parser.add_argument('--layer', type=str, default="-1", help='The layer to be processed. Default is -1')
parser.add_argument('--device', type=str, default="cuda", help='The device to run on. Default is cuda')

args, unknown = parser.parse_known_args()

torch.set_grad_enabled(False)


def gpu_usage():
    def inner():
        if torch.cuda.is_available():
            r = os.popen("nvidia-smi")
            text = r.read()
            r.close()
            return text
        else:
            return "ä»…æ”¯æŒcudaç¯å¢ƒ"

    return inner


def process(model_name, n_inst_train, refusal_dir_coefficient, layer, device, progress=gr.Progress()):
    def p(percentage, message):
        print(f"{percentage * 100}%: {message}")
        progress(percentage, message)

    def clear_gpu_cache():
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    def get_instructions(file_path, column_name):
        dataset = pd.read_csv(file_path)
        instructions = dataset[column_name].tolist()
        train, test = train_test_split(instructions, test_size=0.2, random_state=42)
        return train, test

    def tokenize_instructions(tokenizer, instructions):
        prompts = [tokenizer.apply_chat_template(
            [{"role": "user", "content": instruction}],
            tokenize=False,
            add_generation_prompt=True
        ) for instruction in instructions]
        return tokenizer(prompts, padding=True, truncation=False, return_tensors="pt").input_ids

    def get_orthogonalized_matrix(matrix, vec):
        proj = einops.einsum(matrix, vec.view(-1, 1), '... d_model, d_model single -> ... single') * vec
        return matrix - proj

    try:
        p(0, "æ­£åœ¨åŠ è½½æ¨¡å‹...")

        model = HookedTransformer.from_pretrained(
            model_name,
            device=device,
            n_devices=1,
            dtype=torch.bfloat16,
            default_padding_side='left'
        )
        model.tokenizer.padding_side = 'left'
        model.tokenizer.pad_token = '<|extra_0|>'

        tokenize_instructions_fn = functools.partial(tokenize_instructions, tokenizer=model.tokenizer)
        model.tokenizer.add_special_tokens({'pad_token': '[PAD]'})

        p(0.1, "æ­£åœ¨åŠ è½½æœ‰å®³æ•°æ®åº“...")
        harmful_inst_train, harmful_inst_test = get_instructions("harmful_behaviors.csv", 'goal')

        p(0.15, "æ­£åœ¨åŠ è½½æ— å®³æ•°æ®åº“...")
        harmless_inst_train, harmless_inst_test = get_instructions("./harmless_behaviors", 'instruction')

        p(0.2, "æ­£åœ¨åˆ†è¯æœ‰å®³æ•°æ®åº“...")
        harmful_tokens = tokenize_instructions_fn(instructions=harmful_inst_train[:n_inst_train])

        p(0.25, "æ­£åœ¨åˆ†è¯æ— å®³æ•°æ®åº“...")
        harmless_tokens = tokenize_instructions_fn(instructions=harmless_inst_train[:n_inst_train])

        p(0.3, "æ­£åœ¨è·å–æœ‰å®³æ•°æ®åº“ä¸­é—´è¿‡ç¨‹å‚æ•°...")
        harmful_logits, harmful_cache = model.run_with_cache(harmful_tokens,
                                                             names_filter=lambda hook_name: 'resid' in hook_name)
        harmful_logits.cpu()
        clear_gpu_cache()

        p(0.5, "æ­£åœ¨è·å–æ— å®³æ•°æ®åº“ä¸­é—´è¿‡ç¨‹å‚æ•°...")
        harmless_logits, harmless_cache = model.run_with_cache(harmless_tokens,
                                                               names_filter=lambda hook_name: 'resid' in hook_name)
        harmless_logits.cpu()
        clear_gpu_cache()

        p(0.7, "æ­£åœ¨è®¡ç®—æœ‰å®³ä¸æ— å®³æ¿€æ´»å¹³å‡å·®å¼‚...")
        pos = -1

        refusal_dir: List[Optional[torch.Tensor]] = [None for _ in range(len(model.blocks))]

        if layer == "-2":
            layer = [i for i in range(len(model.blocks))]
        elif layer == "-1":
            layer = [int(len(model.blocks) / 2)]
        elif len(layer.split(",")) != 1:
            layer = [int(i) for i in layer.split(",")]
        else:
            layer = [int(layer)]

        if int(len(model.blocks) / 2) not in layer:
            layer.append(int(len(model.blocks) / 2))

        for i in layer:
            harmful_mean_act = harmful_cache['resid_pre', i][:, pos, :].mean(dim=0)
            harmless_mean_act = harmless_cache['resid_pre', i][:, pos, :].mean(dim=0)
            refusal_dir[i] = harmful_mean_act - harmless_mean_act
            refusal_dir[i] = refusal_dir[i] / refusal_dir[i].norm()
            refusal_dir[i] = refusal_dir[i] * refusal_dir_coefficient

        filtered_tensors = [t for t in refusal_dir if t is not None]
        weights = torch.randn(len(filtered_tensors))
        weights = torch.softmax(weights, dim=0)
        combined_tensor = torch.sum(torch.stack([w * t for w, t in zip(weights, filtered_tensors)]), dim=0)

        p(0.72, "æ­£åœ¨ç§»é™¤åŸæ¨¡å‹å®‰å…¨æ–¹å‘å‘é‡...")

        print(f"æ¨¡å‹æ€»å±‚æ•°ï¼š{len(model.blocks)} å½“å‰æå–ç”¨å±‚æ•°ï¼š{layer}")

        blocks: List[List[Optional[torch.Tensor]]] = [[None, None] for _ in range(len(model.blocks))]

        for i in range(len(model.blocks)):
            blocks[i][0] = get_orthogonalized_matrix(model.blocks[i].attn.W_O, combined_tensor)
            blocks[i][1] = get_orthogonalized_matrix(model.blocks[i].mlp.W_out, combined_tensor)

        model_W_E_data = get_orthogonalized_matrix(model.W_E, combined_tensor)

        p(0.73, "æ­£åœ¨æ¸…ç†å†…å­˜ä¸æ˜¾å­˜...")
        model.cpu()
        gc.collect()
        clear_gpu_cache()

        p(0.75, "æ­£åœ¨åˆ›å»ºåŸæ¨¡å‹å‰¯æœ¬...")
        save_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=device)

        p(0.95, "æ­£åœ¨å†™å…¥æ–°å‘é‡åˆ°æ–°æ¨¡å‹...")
        with torch.no_grad():
            save_model.model.embed_tokens.weight.data = model_W_E_data.contiguous()
            size = blocks[layer[0]][0].shape[-1]
            for i in range(len(blocks)):
                save_model.model.layers[i].self_attn.o_proj.weight.data = blocks[i][0].view(-1, size).T.contiguous()
                save_model.model.layers[i].mlp.down_proj.weight.data = blocks[i][1].T.contiguous()
        save_model.save_pretrained(f"{model_name}-Without-Refusal")
        AutoTokenizer.from_pretrained(model_name).save_pretrained(f"{model_name}-Without-Refusal")

        p(1, "å®Œæˆ")
        save_model = save_model.cpu()
        gc.collect()
        clear_gpu_cache()

        if 'save_model' in locals():
            del save_model

        print("ğŸ¤— åˆ›å»ºæˆåŠŸ\n\nå·²ä¿å­˜æ¨¡å‹åˆ°" + os.getcwd() + f"/{model_name}-Without-Refusal")
        return ["ğŸ¤— åˆ›å»ºæˆåŠŸ\n\nå·²ä¿å­˜æ¨¡å‹åˆ°" + os.getcwd() + f"/{model_name}-Without-Refusal", "å·²æ¸…ç†æ˜¾å¡æ˜¾å­˜å ç”¨"]
    except Exception as e:
        print(e)

        # æ£€æŸ¥å¹¶åˆ é™¤å˜é‡
        for var_name in ['model', 'save_model']:
            if var_name in locals():
                del locals()[var_name]

        # æ¸…ç†æ˜¾å­˜
        clear_gpu_cache()

        return ["ğŸ¤¯ å‘ç”Ÿé”™è¯¯\n\n" + str(e), "å·²æ¸…ç†æ˜¾å¡æ˜¾å­˜å ç”¨"]


if args.cli:
    print("\n\n\n----------CLI Mode----------\n")
    print(f"""Arguments:
    - Model Name: {args.model_name}
    - Number of Training Instances: {args.n_inst_train}
    - Refusal Direction Coefficient: {args.refusal_dir_coefficient}
    - Layer: {args.layer if args.layer != "-1" else "Middle(-1)"}
    - Device: {args.device}
    """)
    print("Continue? (y/n)")
    if input() == "y":
        process(args.model_name, args.n_inst_train, args.refusal_dir_coefficient, args.layer, args.device)
    else:
        print("Use --help for help")
        exit()
else:
    app = gr.Interface(fn=process, inputs=[
        gr.Dropdown(
            OFFICIAL_MODEL_NAMES,
            label="model_name", info="éœ€å¤„ç†æ¨¡å‹ï¼ˆç›®å‰ä»…æ”¯æŒåˆ—è¡¨ä¸­çš„æ¨¡å‹ï¼‰", value="Qwen/Qwen2-1.5B-Instruct"
        ),
        gr.Slider(1, 500, value=32, label="n_inst_train",
                  info="æ‹’ç»æ–¹å‘é’“é±¼ä¾‹å¥æ•°é‡\næ›´åŠ ç²¾å‡†çš„åˆ¤åˆ«æ‹’ç»æ–¹å‘ï¼ˆè¶Šå¤§è¶Šå‡†ç¡®ï¼‰ï¼ˆé«˜æ˜¾å­˜éœ€æ±‚ï¼‰"),
        gr.Slider(0.5, 2, value=1, label="refusal_dir_coefficient",
                  info="å»é™¤æ‹’ç»æ–¹å‘ç³»æ•°ï¼ˆå¢å¼ºå»é™¤æ•ˆæœã€‚ä¼šæ˜¾è‘—å½±å“å¤§æ¨¡å‹è‡ªèº«èƒ½åŠ›ã€‚ï¼‰ï¼ˆé»˜è®¤ä¸º1ï¼‰"),
        gr.Text(label="layer", value="-1", info="æå–ç‰¹å¾å±‚ï¼ˆ-1ä¸ºå–æ¨¡å‹ä¸­é—´å±‚ï¼Œ-2ä¸ºå¯¹æ¯ä¸€å±‚ç‹¬ç«‹è¿›è¡Œå¤„ç†ï¼ˆé«˜æ˜¾å­˜éœ€æ±‚ï¼‰ï¼‰"),
        gr.Text(label="device", value="cuda", info="è¿è¡Œè®¾å¤‡")

    ], outputs=[gr.Text(label="çŠ¶æ€", value="å°±ç»ª"),
                gr.Textbox(label="æ˜¾å¡å®æ—¶çŠ¶æ€",
                           value=gpu_usage(),
                           every=3)], title="rrrLLM Generator",
                       description="é€šè¿‡å¯¹å±‚ä¸­æ£€æµ‹åˆ°çš„æ‹’ç»æ–¹å‘è¿›è¡Œæ¶ˆé™¤ï¼Œå®ç°ç”Ÿæˆæ›´ä½æ‹’ç»å›ç­”ç‡çš„å¤§æ¨¡å‹ã€‚")

    app.launch(share=False, server_port=args.port, server_name=args.host)
